{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression을 python, tensorflow, sklearn으로 각각 구현\n",
    "logistic curve\n",
    ">로지스틱 곡선(曲線): 지수 함수라고도 하며, 갖가지 형태의 성장 모델에 쓰이는 S자형 곡선."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset #\n",
    "\n",
    "# Logistic Regression을 python, tensorflow, sklearn으로 각각 구현\n",
    "# 독립변수 1개\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "\n",
    "####\n",
    "# 수치미분함수를 들고와서 사용한다.\n",
    "# 다변수 수치미분코드\n",
    "def numerical_derivative(f, x):\n",
    "    # f: 미분하려고 하는 다변수 함수\n",
    "    # x : 모든 변수를 포함하고 있어야 한다. ndarray (차원 상관 없이)\n",
    "    \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x) # 미분한 결과를 저장하는 ndarray\n",
    "    \n",
    "    # iterator를 이용해서 입력변수 x에 대해 편미분을 수행\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index # iterator의 현재 index를 추출\n",
    "        \n",
    "        tmp = x[idx]           # 현재 idx의 값을 잠시 보존. delta_x를 이용한 값으로\n",
    "                               # ndarray를 수정한 후 함수값을 계산해야 하기 때문\n",
    "                               # 함수값을 계산한 후 원상복구해야 다음 변수에 대한 편미분을\n",
    "                               # 정상적으로 수행할 수 있다.\n",
    "        x[idx] = tmp + delta_x\n",
    "        fx_plus_delta = f(x)    # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp - delta_x#\n",
    "        fx_minus_delta = f(x)   # f(x - delta_x)\n",
    "    \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "        \n",
    "        x[idx] = tmp # 데이터 원상 복구\n",
    "        \n",
    "        it.iternext()\n",
    "    \n",
    "    return  derivative_x\n",
    "\n",
    "############\n",
    "\n",
    "# 1. Raw Data Loading + Data Preprocessing\n",
    "\n",
    "# 2. Training Data SEt\n",
    "# 지도학습을 하고 있기 때문에 독립변수와 종속변수 (label)로 구분해서 데이터를 준비\n",
    "# 어떤 경우에는 이 두개를 아예 분리해서 제공하는 경우도 있다\n",
    "x_data = np.arange(2,21,2).reshape(-1,1)\n",
    "t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. python 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### python 결과값 ##############\n",
      "공부시간 : [[13]], 결과: (1, array([[0.58167112]]))\n"
     ]
    }
   ],
   "source": [
    "# 3. Weight & bias = > Wx + b\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)  # shape -> (1, ) => b.shape[0]로 접근\n",
    "# 상수 f라서 브로그캐스팅 일어나서 Scalar와 연산 가능\n",
    "\n",
    "# 위에서 정의한 W와 b값 구하여, 최종 목적인 model 완성하기\n",
    "\n",
    "# 4. loss function(손실함수, cost function , 비용함수)\n",
    "# 입력으로 들어온 x_data와 W, b를 이용해서 예측값 계산, t_data 정답과 비교\n",
    "def loss_func(input_obj):\n",
    "    # input_obj : W와 b를 같이 포함하고 있는 ndarray => [W1 W2 W3 b]\n",
    "    num_of_bias = b.shape[0]                                       # num_of_bias : 1\n",
    "    \n",
    "    input_W = input_obj[:-1*num_of_bias].reshape(-1, num_of_bias) # 행렬 연산을 하기 위한 W를 생성\n",
    "    input_b = input_obj[-1*num_of_bias:]                          # bias\n",
    "    \n",
    "    # 모델의 예측값 linear regression modeol((Wx + b))  => sigmoid  적용\n",
    "    z = np.dot(x_data, input_W) + input_b\n",
    "    y = 1 / (1 + np.exp(-1 * z)) # sigmoid\n",
    "    \n",
    "    delta = 1e-7\n",
    "    # 굉장히 작은 값을 이용해서 프로그램이 로그 연산 시 무한대로 발산하는 것을 방지\n",
    "    \n",
    "    # cross entropy\n",
    "    return -np.sum(t_data*np.log(y+delta) + ((1-t_data)*np.log(1-y+delta)))\n",
    "\n",
    "# 5. learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 6. 학습\n",
    "for step in range(30000):\n",
    "    \n",
    "    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0) # [W b] [W1 W2 W3 b]\n",
    "    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)\n",
    "    \n",
    "    num_of_bias = b.shape[0]\n",
    "    \n",
    "    W = W - derivative_result[:-1*num_of_bias].reshape(-1, num_of_bias) # [[W1 W2 W3]]\n",
    "    b = b - derivative_result[-1*num_of_bias:]\n",
    "    \n",
    "# 6. predict => W, b를 다 구해서, Logistic Regression Model을 완성\n",
    "def logistic_predict(x): # 공부한 시간을 입력\n",
    "    z = np.dot(x, W) + b\n",
    "    y = 1 / ( 1 + np.exp(-1*z))\n",
    "    \n",
    "    if y < 0.5:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = 1\n",
    "        \n",
    "    return result, y\n",
    "\n",
    "study_hour = np.array([[13]])\n",
    "result = logistic_predict(study_hour)\n",
    "print('########### python 결과값 ##############')\n",
    "print('공부시간 : {}, 결과: {}'.format(study_hour, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. sklearn 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### python 결과값 ##############\n",
      "공부시간 : [[13]], 결과: [0], [[0.50009391 0.49990609]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model 생성\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Training data set을 이용해서 학습\n",
    "model.fit(x_data, t_data.ravel())\n",
    "\n",
    "study_hour = np.array([[13]])\n",
    "predict_val = model.predict(study_hour)\n",
    "predict_proba = model.predict_proba(study_hour)\n",
    "\n",
    "print('########### sklearn 결과값 ##############')\n",
    "print('공부시간 : {}, 결과: {}, {}'.format(study_hour, predict_val, predict_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### tensorflow 결과값 ##############\n",
      "공부시간 : [13], 결과: [[0.57699466]]\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "X = tf.placeholder(dtype=tf.float32) # 독립변수가 1개인 경우 shape 명시 x\n",
    "T = tf.placeholder(dtype=tf.float32) \n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([1,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = W * X + b # matrix 곱 연산 하지 않나요\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 1e-3).minimize(loss)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    sess.run(train, feed_dict={X: x_data, T: t_data})\n",
    "\n",
    "study_hour = np.array([13])\n",
    "result = sess.run(H, feed_dict={X: study_hour})\n",
    "\n",
    "print('########### tensorflow 결과값 ##############')\n",
    "print('공부시간 : {}, 결과: {}'.format(study_hour, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Variable Logistic Regression\n",
    "- 학습하는 데이터는 GRE (Graduate Record Examination)\n",
    "- GPA (Grade Point Average) 성적\n",
    "- Rank (University Rating)에 대한\n",
    "- 대학원 합격/ 불합격 정보\n",
    "\n",
    "- 내 성적 \\[600. 3.8. 1\\]의 결과\n",
    "- 첫번째 구현은 sklearn으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4)\n",
      "(382, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdi0lEQVR4nO3df7BcdZnn8ffHEAHlZyRgTIjJVgUVKAG9E1R2dhAYCegYp0p3gzOCFFYWCx3YsnYAqxQsiyqnmLXQQUylkCGWLNksRslQQSfLGJFyAiQxBpKIZkFDhiwJEMCIBZPw2T/OCbZ9+97bhHvPOd39eVV13T7nPN154FvdT59fz1e2iYiIaJrX1Z1AREREJylQERHRSClQERHRSClQERHRSClQERHRSAfV9Q8fc8wxnjVrVl3/fCOtW7fuKdtT686jVcZpuIxT72jaWGWcOhtpnGorULNmzWLt2rV1/fONJOk3defQLuM0XMapdzRtrDJOnY00TjnEFxERjZQCFRERjZQCFRERjZQCFRERjZQCFRERjZQCFRERjZQCFRERjTRmgZJ0i6Sdkh4eYbskfV3SVkkbJb1r/NOMkUj6b5I2SXpY0u2SDpE0RdIqSb8q/x7dEn91OVaPSDq3ztwHmaRJkn4m6a4O2/KZaoiMU7262YO6FZg3yvbzgDnlYyHwzdeeVnRD0nTgb4Ah2ycDk4AFwFXAPbbnAPeUy0g6sdx+EsWY3iRpUh25B5cDW0bYls9Uc2ScajRmgbJ9L/DMKCHzgW+7sAY4StK08UowxnQQcKikg4A3AE9QjMmScvsS4CPl8/nAUtsv2n4M2ArMrTbdkDQD+CBw8wgh+Uw1QMapfuNxDmo68HjL8vZy3TCSFkpaK2ntrl27xuGffvWmTJmCpDEfU6ZMqSW/V8P2vwF/D2wDdgDP2f5n4DjbO8qYHcCx5Uu6GqsmjFNLLh0fPe4G4G+Bl0fY3jfj1ONjdQMZp1qNR4Hq9F/WcR5524ttD9kemjq1nv6Nu3fvxvaYj927d9eS36tRnluaD8wG3gK8UdJfj/aSDuuGjVUTxqkll1cercu9StKHgJ22140W1mFdT45Tr45VxqkZxqNAbQeOb1meQXGYKSbeOcBjtnfZ/ndgOfA+4Mn9hxrKvzvL+IxV/c4APizp18BS4CxJ32mLyTjVL+PUAONRoFYAF5ZXtLyH4jDTjnF43xjbNuA9kt6gYh/9bIoTuiuAi8qYi4A7y+crgAWSDpY0m+Lk7gMV5zzQbF9te4btWRQXrPyL7fa93nymapZxaoYxp9uQdDtwJnCMpO3ANcBkANuLgJXA+RQn3F8ALp6oZOOP2b5f0h3AemAv8DNgMXAYsEzSJRRF7GNl/CZJy4DNZfxltvfVknz8EUmXQj5TTZdxqtaYBcr2BWNsN3DZuGUUr4rtayh+NLR6kWJvqlP8dcB1E51XjM32amB1+XxRy/p8phok41SfdJKIiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKiIhGSoGKqJCkQyQ9IOnnkjZJ+lKHmCMl/VNLzMV15BpRt4PqTiBiwLwInGV7j6TJwH2S7ra9piXmMmCz7b+QNBV4RNJttl+qJeOImqRARVTItoE95eLk8uH2MOBwSQIOA54B9laWZERD5BBfRMUkTZK0AdgJrLJ9f1vIjcA7gCeAh4DLbb/c4X0WSlorae2uXbsmOu2IyqVARVTM9j7bpwIzgLmSTm4LORfYALwFOBW4UdIRHd5nse0h20NTp06d2KQHTJfnCs+U9JykDeXji3Xk2s9yiC+iJraflbQamAc83LLpYuAr5eHArZIeA94OPFB9lgOrm3OFAD+x/aEa8hsI2YOKqJCkqZKOKp8fCpwD/KItbBtwdhlzHPA24NEK0xx4Lox1rjAmWApURLWmAT+StBF4kOIc1F2SLpV0aRnzZeB9kh4C7gGutP1UTfkOrC7OFQK8tzwMeLekk6rNsP/lEF9EhWxvBE7rsH5Ry/MngA9UmVcMZ3sfcGq5x/s9SSfbbj0Uux54a3kY8Hzg+8Cc9veRtBBYCDBz5swJz7ufZA8qImIUtp8FVlOcK2xd//z+w4C2VwKTJR3T4fW5mOUApUBFRLTp5lyhpDeX96ohaS7F9+nTFafa13KILyJiuGnAEkmTKArPsv3nCuGVQ7IfBT4taS/we2BBeeVljJOuCpSkecDXgEnAzba/0rb9SOA7wMzyPf/e9j+Oc64REZXo8lzhjRQ3VccEGfMQX/kL4hvAecCJwAWSTmwL29877BTgTOB/SHr9OOcaEREDpJtzUHOBrbYfLZtVLgXmt8Wkd1hERIyrbgrUdODxluXt5bpW6R0WERHjqpsCpQ7r2k8EpndYRESMq24K1Hbg+JblGRR7Sq0uBpaX7UG2Avt7h0VERByQbgrUg8AcSbPLCx8WACvaYtI7rAaS3tbSSXmDpOclXSFpiqRVkn5V/j265TVXS9oq6RFJ59aZf0TEaMYsULb3Ap8BfghsobgfYFN6h9XP9iO2Ty2nbng38ALwPeAq4B7bcyjG4yqA8urLBcBJFHfF31RepRkR0Thd3QdVtvFY2bYuvcOa5Wzg/9r+jaT5FJf7AyyhaNNyJcXVl0ttvwg8JmkrxVWa/1p9uhERo0uro/6xALi9fH6c7R0A5d9jy/XdXJGZqy0johFSoPpAeW7ww8D/Hiu0w7phrVlytWV0a8qUKUj6owcwbN2UKVNqzjR6UXrx9YfzgPW2nyyXn5Q0zfYOSdMo5rOB7q7IjOja7t276ab93P7CFfFqZA+qP1zAHw7vQXGV5UXl84uAO1vWL5B0sKTZFHPXZBrxiGik7EH1OElvAP4c+K8tq78CLJN0CcUtAB8DKK++XAZspmhFdVk5KVtEROOkQPU42y8Ab2pb9zTlfWkd4q8DrqsgtYiI1ySH+CIiopFSoCIiopFSoCIiopFSoCIiopFSoCIiopFSoKKR0qEgInKZeTRSOhRERPagIiok6RBJD0j6uaRNkr40QtyZ5RxfmyT9uOo8I5oge1AR1XoROMv2HkmTgfsk3W17zf4ASUcBNwHzbG+TdOwI7xXR11KgIirk4rjlnnJxcvloP5b5cWC57W3la3YSMYByiC+iYpImSdpA0WV+le3720JOAI6WtFrSOkkXjvA+mbcr+loKVETFbO+zfSrFdCdzJZ3cFnIQ8G7gg8C5wBckndDhfTJvV/S1FKiImth+FlgNzGvbtB34ge3f2X4KuBc4pdrsIuqXAhVRIUlTy4sgkHQocA7wi7awO4E/lXRQOZ3K6cCWShONaIBcJBFRrWnAEkmTKH4gLrN9l6RLAWwvsr1F0g+AjcDLwM22H64v5Yh6pEBFVMj2RuC0DusXtS1fD1xfVV7xxyQdQnFo9WCK78k7bF/TFiPga8D5wAvAJ22vrzrXfpYCFREx3Jj3qwHnAXPKx+nAN8u/MU5yDioioo0LY92vNh/4dhm7BjhK0rQq8+x3KVARER10cb/adODxluXt5br296n9frVebb6cAhUR0UEX96t16lQ8rMNxE+5X2998eazH7t27a8lvJClQERGjGON+teNblmcAT1ST1WBIgYqIaNPl/WorgAtVeA/wnO0d1Wba33IVX0TEcGPerwaspLjEfCvFZeYX15Vsv0qBioho0839amVn+suqzGvQ5BBfREQ0UgpUREQ0UgpUREQ0UgpUREQ0UgpUREQ0UgpUREQ0UlcFStI8SY9I2irpqhFizpS0QdImST8e3zQjImLQjHkfVHmj2jeAP6do7fGgpBW2N7fEHAXcBMyzvU3SsROUb0REDIhu9qDmAlttP2r7JWApRZv5Vh8HltveBmB75/imGRERg6abAtVNS/kTgKMlrZa0TtKFnd6oCW3nIyKiN3RToLppKX8Q8G7gg8C5wBcknTDsRQ1oOx8REb2hm1583bSU3w48Zft3wO8k3QucAvxyXLKMiIiB080e1IPAHEmzJb0eWEDRZr7VncCfSjpI0huA04Et45tqREQMkjH3oGzvlfQZ4IfAJOAW25ta287b3iLpB8BG4GXgZtsPT2TiERHR37qabsP2Soq5T1rXLWpbvh64fvxSi4iIQZb5oCLigPmaI+DaI7uLi3iVUqAiKiTpEOBe4GCKz98dtq8ZIfZPgDXAf7F9R3VZdk9fep5i3r4x4iR87cTnE/0lBSqiWi8CZ9neI2kycJ+ku22vaQ0qO7j8HcW534iBlGaxERVyYU+5OLl8dNoF+SzwXSBdWWJgpUD1OElHSbpD0i8kbZH0XklTJK2S9Kvy79Et8VeXTX8fkXRunbkPKkmTJG2gKD6rbN/ftn068JfAog4vjxgYKVC972vAD2y/neLm6C3AVcA9tucA95TLSDqR4j62k4B5wE3loaSokO19tk+luOl9rqST20JuAK60vW+090nrsOh3KVA9TNIRwH8CvgVg+yXbz1I0811Shi0BPlI+nw8stf2i7ceArRTNgKMG5Vitpvix0GoIWCrp18BHKX5IfKTD69M6LPpaLpLobf8B2AX8o6RTgHXA5cBxtncA2N7RMv3JdIqrwvbr1PgXSQuBhQAzZ86cuOxH0a+XL0uaCvy77WclHQqcQ3ExxCtsz26JvxW4y/b3q8wzoglSoHrbQcC7gM/avl/S1ygP542gm8a/2F4MLAYYGhoa+xriCdDHly9PA5aUh1ZfByyzfVdrZ5Zas4tokBSo3rYd2N5ykv0OigL1pKRp5d7TNP5wJVg3jX9jAtneCJzWYX3HwmT7kxOdU0RT5RxUD7P9/4DHJb2tXHU2sJmime9F5bqLKJr5Uq5fIOlgSbOBOcADFaYcEdG17EH1vs8Ct5Wd5h8FLqY8dCTpEmAb8DGAssnvMooithe4bKwrxSIi6pIC1eNsb6C46qvd2SPEXwdcN5E5RUSMhxzii4iIRkqBiohoI+l4ST8qu7NsknR5h5gzJT0naUP5+GIdufazHOKLiBhuL/A52+slHQ6sk7TK9ua2uJ/Y/lAN+Q2E7EFFRLSxvcP2+vL5bylaiA27qT0mVgpURMQoJM2iuHft/g6b3yvp55LulnTSCK9Pz8QDlAIVETECSYdRTHtyhe3n2zavB95q+xTgH4Dvd3qP9Ew8cClQEREdlBNKfhe4zfby9u22n98/t5ftlcBkScdUnGZfS4GKiGgjSRSzBGyx/dURYt5cxiFpLsX36dPVZdn/chVfRMRwZwCfAB4qJ5cE+DwwE17pnfhR4NOS9gK/Bxa4mw7H0bUUqIiINrbvo3P3/9aYG4Ebq8loMOUQX0RENFIKVERENFIKVERENFIKVERENFIKVERENFIKVERENFIKVERENFIKVERENFIKVERENFIKVERENFIKVERENFIKVERENFIKVESFJB0i6YFyFtZNkr7UIeavJG0sHz+VdEoduUbULd3MI6r1InCW7T3lhHj3Sbrb9pqWmMeAP7O9W9J5wGLg9DqSjahTV3tQkuZJekTSVklXjRL3J5L2Sfro+KUY0T9c2FMuTi4fbov5qe3d5eIaYEaFKUY0xpgFStIk4BvAecCJwAWSThwh7u+AH453khH9RNKkchK8ncAq2/ePEn4JcPcI77NQ0lpJa3ft2jUBmUbUq5s9qLnAVtuP2n4JWArM7xD3WeC7FB+6iBiB7X22T6XYM5or6eROcZLeT1GgrhzhfRbbHrI9NHXq1AnLN6Iu3RSo6cDjLcvby3WvkDQd+Etg0WhvlF98EX9g+1lgNTCvfZukdwI3A/NtP11tZhHN0E2B6jTtsduWbwCutL1vtDfKL74YdJKmSjqqfH4ocA7wi7aYmcBy4BO2f1l5khEN0c1VfNuB41uWZwBPtMUMAUslARwDnC9pr+3vj0eSEX1kGrCkPGf7OmCZ7bskXQpgexHwReBNwE3lZ2qv7aG6Eo6oSzcF6kFgjqTZwL8BC4CPtwbYnr3/uaRbgbtSnCKGs70ROK3D+kUtzz8FfKrKvCKaaMwCZXuvpM9QXJ03CbjF9qa2X3wRERHjqqsbdW2vBFa2retYmGx/8rWnFRERgy6dJCLiNSnPk43q6KOPriCT6DcpUBFxwOz2C3qLgtVpfcSrlQIVjZVf5hGDLQUqGim/zCMi021EREQjpUBFREQjpUBFRLSRdLykH0naUk4seXmHGEn6ejkN0UZJ76oj136Wc1AREcPtBT5ne72kw4F1klbZ3twScx4wp3ycDnyTTCw5rrIHFRHRxvYO2+vL578FttA2iwPFtEPfLiehXAMcJWlaxan2texB9ThJvwZ+C+yjbCoqaQrwv4BZwK+B/7x/hlZJV1PMMbQP+BvbmWAyYhSSZlH0T2yfWHKkqYh2tL1+IbAQYObMmROW52h8zRFw7ZHdxTVIClR/eL/tp1qWrwLusf0VSVeVy1eWMyEvAE4C3gL8H0knjDVNSsSgknQYxUSsV9h+vn1zh5cMuw/C9mJgMcDQ0FA990lc+9ywVb1w20YO8fWn+cCS8vkS4CMt65faftH2Y8BWihmTI6KNpMkUxek228s7hHQzFVG8BilQvc/AP0taVx5KADjO9g4ojqUDx5brx5wdGTLzcYSKNibfArbY/uoIYSuAC8ur+d4DPLf/cxfjI4f4et8Ztp+QdCywStIvRontnUMSEfU6A/gE8JCkDeW6zwMz4ZXZHFYC51MciXgBuLj6NPtbClSPs/1E+XenpO9RHLJ7UtI02zvKq4p2luE5JBHRBdv30fkHXWuMgcuqyWgw5RBfD5P0xvIeDSS9EfgA8DDFoYeLyrCLgDvL5yuABZIOLmdIngM8UG3WERHdyR5UbzsO+F7Z9fsg4H/a/oGkB4Flki4BtgEfAyhnQl4GbKa4EfGyXMEXEU2VAtXDbD8KnNJh/dPA2SO85jrguglOLSLiNcshvoiIaKQUqIiIaKQUqIiIaKQUqIiIaKQUqIgKSTpE0gOSfl7OM/SlDjGZZyiCXMUXUbUXgbNs7yl7vd0n6e5yuob9Ms9QBNmDiqhUOXfQnnJxcvlobyeVeYYiSIGKqJykSWV/t53AKtvdzjMUMVBSoCIqZnuf7VMpeiHOlXRyW0hXTX3TdT76XQpURE1sPwusBua1beqqqa/txbaHbA9NnTp1otKMqE0KVESFJE2VdFT5/FDgHKB9ipTMMxRBruKLqNo0YImkSRQ/EJfZvkvSpZB5hiJapUBFVMj2RuC0DusXtTzPPEMR5BBfREQ0VApUREQ0UgpUREQ0UgpUREQ0UgpUREQ0UlcFStI8SY+U3ZWv6rD9r8quyxsl/VTSsGnIIyIiXo0xC1R5v8Y3KDosnwhcIOnEtrDHgD+z/U7gy8Di8U40IiIGSzd7UHOBrbYftf0SsJSi2/IrbP/U9u5ycQ1Fa5aIiIgD1k2BerWdlS8B7u60Ic0tIyKiW90UqK46KwNIej9Fgbqy0/Y0t4yIiG510+qoq87Kkt4J3AycZ/vp8UkvIiIGVTd7UA8CcyTNlvR6YAFFt+VXSJoJLAc+YfuX459mREQMmjELlO29wGeAHwJbKLovb5J06f4OzMAXgTcBN0naIGnthGUcEVEBSbdI2inp4RG2nynpufI7b4OkL1adY7/rqpu57ZUUUwC0rmvtvvwp4FPjm1pERK1uBW4Evj1KzE9sf6iadAZPOklERHRg+17gmbrzGGQpUBERB+69kn4u6W5JJ9WdTL/JhIUREQdmPfBW23sknQ98H5jTHiRpIbAQYObMmZUm2OuyBxURcQBsP297T/l8JTBZ0jEd4nL/5wFKgYqIOACS3ixJ5fO5FN+nuQd0HOUQX0REB5JuB84EjpG0HbgGmAyvXMX8UeDTkvYCvwcW2O7YZScOTApUREQHti8YY/uNFJehxwTJIb6IiGikFKiIiGikFKiIiGikFKiIiGikFKiICkk6XtKPJG2RtEnS5R1ijpT0T2WHgk2SLq4j14i65Sq+iGrtBT5ne72kw4F1klbZ3twScxmw2fZfSJoKPCLpNtsv1ZJxRE2yBxVRIds7bK8vn/+WYgqb6e1hwOHlTaCHUTQs3VtpohENkAIVURNJs4DTgPvbNt0IvINi5uqHgMttv9zh9QslrZW0dteuXROdbkTlUqAiaiDpMOC7wBW2n2/bfC6wAXgLcCpwo6Qj2t8jPd6i36VA9QFJkyT9TNJd5fIUSask/ar8e3RL7NWStkp6RNK59WU9uCRNpihOt9le3iHkYmC5C1uBx4C3V5ljRBOkQPWHyynOZex3FXCP7TnAPeUykk4EFgAnAfOAmyRNqjjXgVaeV/oWsMX2V0cI2wacXcYfB7wNeLSaDCOaIwWqx0maAXwQuLll9XxgSfl8CfCRlvVLbb9o+zFgKzC3olSjcAbwCeAsSRvKx/mSLpV0aRnzZeB9kh6i+IFxpe2n6ko4oi65zLz33QD8LXB4y7rjbO+A4qoxSceW66cDa1ritjP8CrJMsDaBbN8HaIyYJ4APVJNRRHNlD6qHSfoQsNP2um5f0mHdsOkBcvI9Ipoge1C97Qzgw+V004cAR0j6DvCkpGnl3tM0YGcZvx04vuX1MyguZY6IaJzsQfUw21fbnmF7FsXFD/9i+6+BFcBFZdhFwJ3l8xXAAkkHS5oNzAEeqDjtiIiuZA+qP30FWCbpEoorwj4GYHuTpGXAZorOBJfZ3ldfmhERI0uB6hO2VwOry+dPU16m3CHuOuC6yhKLiDhAOcQXERGNlAIVERGNlAIVERGNlAIVERGNlAIVERGNlAIVERGNlAIVERGNlAIVERGNlAIVERGNlAIVERGNlAIVERGN1FWBkjRP0iOStkq6qsN2Sfp6uX2jpHeNf6oREdWRdIuknZIeHmF7vvcm2JgFStIk4BvAecCJwAWSTmwLO49i6oY5FDOxfnOc84yIqNqtwLxRtud7b4J1swc1F9hq+1HbLwFLgfltMfOBb7uwBjiqnCgvIqIn2b4XeGaUkHzvTbBuptuYDjzesrwdOL2LmOnAjtYgSQspfmkwc+bMV5vruPA1R8C1R3YXF7WT1HHZHjZTfdRopHGCvh6rnvnea8llxOUmjlM3BUod1rX/l3QTg+3FwGKAoaGhev5vXPtcLf9sHJgmfmhiuAEdp9753vtDLnX+869aN4f4tgPHtyzPAJ44gJiIiH6S770J1k2BehCYI2m2pNcDC4AVbTErgAvLq1reAzxne0f7G0VE9JF8702wMQ/x2d4r6TPAD4FJwC22N0m6tNy+CFgJnA9sBV4ALp64lCMiJp6k24EzgWMkbQeuASZDvveq0s05KGyvpBiM1nWLWp4buGx8U4uIqI/tC8bYnu+9CZZOEhEVknS8pB9J2iJpk6TLR4g7U9KGMubHVecZ0QRd7UFFxLjZC3zO9npJhwPrJK2yvXl/gKSjgJuAeba3STq2plwjapU9qIgK2d5he335/LfAFop7Z1p9HFhue1sZt7PaLCOaIQUqoiaSZgGnAfe3bToBOFrSaknrJF04wusXSlorae2uXbsmONuI6qVARdRA0mHAd4ErbD/ftvkg4N3AB4FzgS9IOqH9PWwvtj1ke2jq1KkTnnNE1XIOKqJikiZTFKfbbC/vELIdeMr274DfSboXOAX4ZYVpRtROdbW+kLQL+E0t//hwxwBP1Z0E8FbbjfopnHHq6IDHSUXzsyXAM7avGCHmHcCNFHtPrwceABbY7jjtQ/majFNnjfpMZZxG1HGcaitQTSJpre2huvOI0fXDOEn6j8BPgIeAl8vVnwdmwh/uL5T03ylu/HwZuNn2DZUne4D6YZwGQS+MUw7xRVTI9n10bjLaHnc9cP3EZxTRXLlIIiIiGikFqrC47gSiKxmn3pBx6g2NH6ecg4qIiEbKHlRERDRSClRERDTSQBcoSbdI2ilpxPtLon4Zp96QceoNvTROA12ggFuBeXUnEWO6lYxTL7iVjFMvuJUeGaeBLlC27wWeqTuPGF3GqTdknHpDL43TQBeoiIhorhSoiIhopBSoiIhopBSoiIhopIEuUJJuB/4VeJuk7ZIuqTunGC7j1BsyTr2hl8YprY4iIqKRBnoPKiIimisFKiIiGikFKiIiGikFKiIiGikFKiIiGikFKiIiGikFKiIiGun/A5C+pqHcQxrhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Raw Data Loading\n",
    "df = pd.read_csv('./data/admission.csv')\n",
    "\n",
    "# 2. 결측치 확인\n",
    "df.isnull().sum()\n",
    "# 결측치는 없다.\n",
    "\n",
    "# 3. 이상치를 확인해서 있으면 제거\n",
    "# fig = plt.figure()\n",
    "# fig_admin = fig.add_subplot(1, 4, 1)\n",
    "# fig_gre = fig.add_subplot(1, 4, 2)\n",
    "# fig_gpa = fig.add_subplot(1, 4, 3)\n",
    "# fig_rank = fig.add_subplot(1, 4, 4)\n",
    "\n",
    "# fig_admin.boxplot(df['admit'])\n",
    "# fig_gre.boxplot(df['gre'])\n",
    "# fig_gpa.boxplot(df['gpa'])\n",
    "# fig_rank.boxplot(df['rank'])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "print(df.shape)\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "for col in df.columns:\n",
    "    outliers = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]\n",
    "    df = df.loc[~df[col].isin(outliers)]\n",
    "    \n",
    "print(df.shape)\n",
    "\n",
    "# 이상치를 확인해서 있으면 제거\n",
    "fig = plt.figure()\n",
    "fig_admin = fig.add_subplot(1, 4, 1)\n",
    "fig_gre = fig.add_subplot(1, 4, 2)\n",
    "fig_gpa = fig.add_subplot(1, 4, 3)\n",
    "fig_rank = fig.add_subplot(1, 4, 4)\n",
    "\n",
    "fig_admin.boxplot(df['admit'])\n",
    "fig_gre.boxplot(df['gre'])\n",
    "fig_gpa.boxplot(df['gpa'])\n",
    "fig_rank.boxplot(df['rank'])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show() # 이상치 제거 됨\n",
    "\n",
    "# 4. Training Data Set\n",
    "# admin 빼고 세개를 training data로 쓴다, inplace=False -> 원본 삭제x\n",
    "# values : dataframe의 값만 numpy화\n",
    "x_data= df.drop('admit', axis=1, inplace=False).values\n",
    "# df['amin']: Series, values: vector\n",
    "# reshape(-1, 1): 2차원 matrix화 \n",
    "t_data = df['admit'].values.reshape(-1, 1)\n",
    "\n",
    "# 5. 정규화\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data) # for python, tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. sklearn을 이용한 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### sklearn 결과값 ##############\n",
      "나의 지원정보 : [[600.    3.8   1. ]], 결과: [1], [[0.43740782 0.56259218]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn을 이용한 구현\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(x_data, t_data.ravel())\n",
    "\n",
    "my_score = np.array([[600, 3.8, 1]])\n",
    "predict_val = model.predict(my_score) # 0 or 1\n",
    "predict_proba = model.predict_proba(my_score) # (불합격할 확률, 합격할 확률)\n",
    "\n",
    "print('########### sklearn 결과값 ##############')\n",
    "print('나의 지원정보 : {}, 결과: {}, {}'.format(my_score, predict_val, predict_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. tensorflow을 이용한 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\j828h\\anaconda3\\envs\\data_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W : [[-1.1890466 ]\n",
      " [ 1.0526935 ]\n",
      " [ 0.20298532]], b : [-0.9008117], loss: 0.6500018239021301\n",
      "W : [[-1.1773398 ]\n",
      " [ 1.0588218 ]\n",
      " [ 0.19310416]], b : [-0.8966231], loss: 0.6490359306335449\n",
      "W : [[-1.1659471 ]\n",
      " [ 1.0645823 ]\n",
      " [ 0.18302959]], b : [-0.892958], loss: 0.6481098532676697\n",
      "W : [[-1.1548606 ]\n",
      " [ 1.0700147 ]\n",
      " [ 0.17279172]], b : [-0.8897562], loss: 0.6472181677818298\n",
      "W : [[-1.1440902 ]\n",
      " [ 1.0751681 ]\n",
      " [ 0.16241725]], b : [-0.88696694], loss: 0.6463566422462463\n",
      "W : [[-1.1335225 ]\n",
      " [ 1.0801057 ]\n",
      " [ 0.15192787]], b : [-0.8845548], loss: 0.6455176472663879\n",
      "W : [[-1.1231513 ]\n",
      " [ 1.0847548 ]\n",
      " [ 0.14135002]], b : [-0.8824785], loss: 0.6447002291679382\n",
      "W : [[-1.1130643 ]\n",
      " [ 1.0892022 ]\n",
      " [ 0.13069385]], b : [-0.8806864], loss: 0.6439041495323181\n",
      "W : [[-1.1030507 ]\n",
      " [ 1.0934937 ]\n",
      " [ 0.11998824]], b : [-0.87915826], loss: 0.6431209444999695\n",
      "W : [[-1.0933387 ]\n",
      " [ 1.0975642 ]\n",
      " [ 0.10923773]], b : [-0.87785417], loss: 0.6423577666282654\n",
      "########### tensorflow 결과값 ##############\n",
      "나의 지원정보 : [[600.    3.8   1. ]], 결과: [[0.37062365]]\n"
     ]
    }
   ],
   "source": [
    "# tensorflow을 이용한 구현\n",
    "\n",
    "# plceholder\n",
    "# 행 미정, 열 3열 (독립변수 3개)\n",
    "X = tf.placeholder(shape=[None, 3], dtype=tf.float32) # 독립변수의 데이터를 받기 위한 plcaholder\n",
    "T = tf.placeholder(shape=[None, 1], dtype=tf.float32) # 종속변수 (label)의 데이터를 받기 위한 placeholder\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(X, W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 1e-4).minimize(loss)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train, W, b, loss],\n",
    "                                         feed_dict={X:norm_x_data, T:t_data})\n",
    "    \n",
    "    if step % 3000 == 0:\n",
    "        print('W : {}, b : {}, loss: {}'.format(W_val, b_val, loss_val))\n",
    "\n",
    "my_score = np.array([[600, 3.8, 1]])\n",
    "scaled_my_score = scaler_x.transform(my_score)\n",
    "result = sess.run(H, feed_dict={X:scaled_my_score})\n",
    "\n",
    "print('########### tensorflow 결과값 ##############')\n",
    "print('나의 지원정보 : {}, 결과: {}'.format(my_score, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admit    0\n",
      "gre      0\n",
      "gpa      0\n",
      "rank     0\n",
      "dtype: int64\n",
      "W: [[ 0.93438536]\n",
      " [ 0.44834182]\n",
      " [-0.37441477]], b: [-1.961393], loss: 0.6361476182937622\n",
      "W: [[ 0.95522255]\n",
      " [ 0.47153884]\n",
      " [-0.36583152]], b: [-1.9277273], loss: 0.6288771033287048\n",
      "W: [[ 0.97424644]\n",
      " [ 0.49282712]\n",
      " [-0.3586862 ]], b: [-1.8970697], loss: 0.6228521466255188\n",
      "W: [[ 0.99157554]\n",
      " [ 0.5123314 ]\n",
      " [-0.3528846 ]], b: [-1.8692309], loss: 0.6178829073905945\n",
      "W: [[ 1.0073327 ]\n",
      " [ 0.53018177]\n",
      " [-0.34832877]], b: [-1.8440022], loss: 0.6137986183166504\n",
      "W: [[ 1.021644 ]\n",
      " [ 0.5465024]\n",
      " [-0.3449212]], b: [-1.8211789], loss: 0.6104500889778137\n",
      "W: [[ 1.034637  ]\n",
      " [ 0.56142265]\n",
      " [-0.34256384]], b: [-1.8005706], loss: 0.607708752155304\n",
      "W: [[ 1.0464036 ]\n",
      " [ 0.5750613 ]\n",
      " [-0.34116307]], b: [-1.7819772], loss: 0.6054659485816956\n",
      "W: [[ 1.057076  ]\n",
      " [ 0.58753645]\n",
      " [-0.3406295 ]], b: [-1.7652164], loss: 0.6036287546157837\n",
      "W: [[ 1.0667409 ]\n",
      " [ 0.5989521 ]\n",
      " [-0.34087968]], b: [-1.7501303], loss: 0.6021220088005066\n",
      "####### tensorflow 결과값 #########\n",
      "내 지원정보 : [[600.    3.8   1. ]], 결과 : [[0.3473347]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Raw Data Loading\n",
    "df = pd.read_csv('./data/admission.csv')\n",
    "\n",
    "# 2. 결측치 확인\n",
    "print(df.isnull().sum()) # 결측치 없음\n",
    "\n",
    "# 3. 이상치 제거\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "for col in df.columns:\n",
    "    outliers = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]\n",
    "    df = df.loc[~df[col].isin(outliers)]\n",
    "    \n",
    "# 4. Training Data Set\n",
    "x_data = df.drop('admit', axis=1, inplace=False).values\n",
    "t_data = df['admit'].values.reshape(-1, 1)\n",
    "\n",
    "# 5. hypothesis\n",
    "logit = tf.matmul(X, W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# 6. loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# 7. train node\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "\n",
    "# 8. session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 9. 학습\n",
    "for step in range(30000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train, W, b, loss],\n",
    "                                        feed_dict={X:norm_x_data, T:t_data})\n",
    "    if step % 3000 == 0:\n",
    "        print('W: {}, b: {}, loss: {}'.format(W_val, b_val, loss_val))\n",
    "        \n",
    "# 10. 정확도 (Accuracy) 측정\n",
    "predict = tf.cast(H >= 0.5, dtype=tf.float32) # True => 1.0, False -> 0\n",
    "correct = tf.equal(predict, T)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# 11. prediction\n",
    "my_score = np.array([[600, 3.8, 1]])\n",
    "scaled_my_score = scaler_x.transform(my_score)\n",
    "\n",
    "result = sess.run(H, feed_dict={X:scaled_my_score})\n",
    "print('####### tensorflow 결과값 #########')\n",
    "print('내 지원정보 : {}, 결과 : {}'.format(my_score,result)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.img](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FvTHbU%2FbtqJQHWomPI%2FaJAtsP4YfcgkJehkcJYoK0%2Fimg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
