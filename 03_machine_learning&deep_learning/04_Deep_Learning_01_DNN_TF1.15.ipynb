{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "-   뇌의 기능 (뉴런)을 모델링한 하나의 학습 기계\n",
    "-   다수의 신호를 입력받아서 하나의 신호를 출력하는 구조\n",
    "## 활성화 함수 (Activation function)\n",
    "\n",
    "-   다음 신경망에 정할지, 일정 수준이 넘는지 판단하는 함수\n",
    "\n",
    "\n",
    "### Perceptron과 논리 GATE\n",
    "-   이런 Perceptron을 이용하면 사람처럼 생각하는 기계를 만들 수 있을 것이다\n",
    "-   Perceptron으로 AND, OR, NAND, XOR GATE을 학습, 판단\n",
    "    -   XOR은 하나의 Perceptron으로 구현할 수 없는 문제 발생\n",
    "    -   MLP (Multi Layer Perceptron)으로 가능하나 너무 어려워서 학습이 거의 안됨\n",
    "    \n",
    "### GATE 연산 수행하는 Logistic Regression 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "WARNING:tensorflow:From C:\\Users\\j828h\\anaconda3\\envs\\data_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "loss: 0.7499009370803833\n",
      "loss: 0.6953208446502686\n",
      "loss: 0.6932694315910339\n",
      "loss: 0.6931568384170532\n",
      "loss: 0.6931479573249817\n",
      "loss: 0.6931472420692444\n",
      "loss: 0.6931471824645996\n",
      "loss: 0.6931471824645996\n",
      "loss: 0.6931471824645996\n",
      "loss: 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "# TF 1.15 버전\n",
    "# GATE 연산 수행하는 Logistic Regression 구현\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(tf.__version__) # 1.15.0\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]], dtype=np.float32)\n",
    "\n",
    "# # AND GATE\n",
    "#  t_data = np.array([[0], [0], [0], [1]], dtype=np.float32) # 2차원\n",
    "\n",
    "# #OR GATE\n",
    "# t_data = np.array([[0], [1], [1], [1]], dtype=np.float32) # 2차원\n",
    "\n",
    "# # NAND GATE\n",
    "# t_data = np.array([[1], [1], [1], [0]], dtype=np.float32) # 2차원\n",
    "\n",
    "# XOR GATE\n",
    "t_data = np.array([[0], [1], [1], [0]], dtype=np.float32) # 2차원\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "# 독립변수 2, 종속변수 1개 (= Output 1개 = Logistic 1개)\n",
    "W = tf.Variable(tf.random.normal([2, 1], name='weight')) \n",
    "b = tf.Variable(tf.random.normal([1], name='bias'))   # bias는 1개 \n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X, W) + b # Linear Regression\n",
    "H = tf.sigmoid(logit) # 0~1로 표현, 활성화 함수\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    # train 학습 -  결과값 확인x, loss는 확인\n",
    "    _, loss_val = sess.run([train, loss],\n",
    "                           feed_dict={X:x_data,\n",
    "                                      T:t_data}) \n",
    "    \n",
    "    if step % 3000 == 0:\n",
    "        print('loss: {}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.50      0.40         2\n",
      "         1.0       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.25         4\n",
      "   macro avg       0.17      0.25      0.20         4\n",
      "weighted avg       0.17      0.25      0.20         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 성능평가 (Accuracy)\n",
    "\n",
    "# tf.cast : boolean값을 0,1 실수로 변환\n",
    "accuracy = tf.cast(H >= 0.5, dtype=tf.float32)\n",
    "result = sess.run(accuracy, feed_dict={X: x_data})\n",
    "\n",
    "# classification_report(정답, 예측값) - 1차원으로 변환하여 입력\n",
    "print(classification_report(t_data.ravel(), result.ravel()))\n",
    "\n",
    "#  accuracy                           1.00  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATE 연산 수행하는 Deep Learning 구현 \n",
    "### DNN (Deep Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "loss: 1.1665525436401367\n",
      "loss: 0.2371227890253067\n",
      "loss: 0.12380000948905945\n",
      "loss: 0.08038771897554398\n",
      "loss: 0.0587424710392952\n",
      "loss: 0.04602735862135887\n",
      "loss: 0.03773375600576401\n",
      "loss: 0.03192317113280296\n",
      "loss: 0.027637198567390442\n",
      "loss: 0.02435065060853958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         2\n",
      "         1.0       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         4\n",
      "   macro avg       1.00      1.00      1.00         4\n",
      "weighted avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# TF 1.15 버전\n",
    "# GATE 연산 수행하는 Deep Learning 구현\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# print(tf.__version__) # 1.15.0\n",
    "\n",
    "# Training Data Set\n",
    "x_data = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]], dtype=np.float32)\n",
    "\n",
    "# XOR GATE\n",
    "t_data = np.array([[0], [1], [1], [0]], dtype=np.float32) # 2차원\n",
    "\n",
    "### placeholder => Input Layer 역할\n",
    "X = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "\n",
    "\n",
    "### Hidden Layer 역할\n",
    "# Weight & bias\n",
    "# 독립변수 2, 종속변수 100 => Logistic 100개\n",
    "W2 = tf.Variable(tf.random.normal([2, 100], name='weight2')) \n",
    "b2 = tf.Variable(tf.random.normal([100], name='bias2'))     \n",
    "layer2 = tf.sigmoid(tf.matmul(X, W2) + b2) # layer2의 결과값 100개\n",
    "\n",
    "# 입력값 100개, 로지스틱 6개\n",
    "W3 = tf.Variable(tf.random.normal([100, 6], name='weight3')) \n",
    "b3 = tf.Variable(tf.random.normal([6], name='bias3'))     \n",
    "layer3= tf.sigmoid(tf.matmul(layer2, W3) + b3) # layer3의 결과값 6개\n",
    "\n",
    "\n",
    "### Output Layer => 최종 결과물은 0 아니면 1 한개\n",
    "W4 = tf.Variable(tf.random.normal([6, 1], name='weight4')) \n",
    "b4 = tf.Variable(tf.random.normal([1], name='bias4'))     \n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer3, W4) + b4 # Linear Regression\n",
    "H = tf.sigmoid(logit)    # 0~1로 표현, 활성화 함수\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    # train 학습 -  결과값 확인x, loss는 확인\n",
    "    _, loss_val = sess.run([train, loss],\n",
    "                           feed_dict={X:x_data,\n",
    "                                      T:t_data}) \n",
    "    \n",
    "    if step % 3000 == 0:\n",
    "        print('loss: {}'.format(loss_val))\n",
    "        \n",
    "        \n",
    "## 성능평가 (Accuracy)\n",
    "\n",
    "# tf.cast : boolean값을 0,1 실수로 변환\n",
    "accuracy = tf.cast(H >= 0.5, dtype=tf.float32)\n",
    "result = sess.run(accuracy, feed_dict={X: x_data})\n",
    "\n",
    "# classification_report(정답, 예측값) - 1차원으로 변환하여 입력\n",
    "print(classification_report(t_data.ravel(), result.ravel()))\n",
    "\n",
    "#  accuracy                           1.00  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
